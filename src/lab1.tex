\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}

% Title Page
\title{Lab 1 Report}
\author{James Danylik}


\begin{document}
\maketitle

\section{Introduction}
 As a newcomer to parallel programming, I set realtively modest goals for this lab, and sought simply to attain the 50\% speedup from the TA's baseline of 12.7x. I elected to follow the methods we established in class to accomplish this goal; find the bottleneck, partition, 
 agglomerate, establish conmunication, and map results.

\section{Methods and Challenges}
To do this, I first compiled the program along with gprof's profiling flag (-pg), and generated the corapsonding data that gprof needs to operate.
Once this was done, it was a simple matter of running the example data sets through the sequential version of DWT provided.  This produced
easy to read timing data that analyzed where time was being spent in the program's exectution.

The timing data indicated that the real work was being split primarily between two functions fwt97\_pd and the containing cdf97.  I began by trying
to implement the lowest level function, which was fwt\_97pdf.  This accounted for around 40\% of the overall program runtime.  However, my efforts
to parallelize the function, while successful, where poorly planned; this result in a substational net slowdown of the program.  I tried playing
with several different implementations of this section; all with either the same results, or worse, incorrect operation.  

After some time, it occured to me to check where the rest of the execution time was going.  My profiling data actually indicated that the containing
cdf97 function was taking the most operating time of all!  How could this be?  With the only looping structure inside the function being levels,
and no major operations done within, where was the time going?  Then, I realized the loop containing the 3d DWT transformation was contained within
the main rountine for that program, and was in fact composed of independent operations on seperate 2d DWT slices.  This gave me the idea to partion these
2d slices amoung the threads, rather then seeking to parallelize the much more complicated fwt97\_pd with its many C macros.  I tried removing my 
previous attempts and replacing them with a simple parallelization based on this observation.  I was quite surprised to find the program speedup to
be floating right above 50\%.

I proceeded to test several scheduling options and hone in on the best, but the crowded environment of the CS133 server made true profiling difficult;
though my code had been running very stablely when the server was deserted late at night, whenever any real population of students would use the 
server, tests would start to vary wildly with server load, making benchmaarking difficult.  As a baseline for measurement, the server load was
maintaining a solid 16, and returning a full 12s for the sequential instead of the expected 3s.
 
\section{Results}

 \begin{center}
\begin{tabular}{ r | c  c  c  c  c }
 Threads & 1 & 2 & 4 & 8 & 16 \\ \hline 
 Runtime & 3.67s  & 2.02s  & 1.21s  & 0.77s  & 0.44s  \\
 Speedup & 1x & 1.81x & 3.03x & 4.76x & 8.34x \\
\end{tabular}
 \end{center}

While I am happy with the simplicity of the solution I found, especially in regards to the ratio of its simplicity to its effects, performance was
difficult to measure with any stability due to the server performance issues mentioned above.  For the sake of these results, I am assuming the TA's runtime to be a low load run,
and thus entering the best runtimes achieved by my alogirthm, which occur when the server has the lowest load.  

To improve benchmarking in the future, one might keep remeasuring the baseline sequential performance once every some interval of time.  This had the advantage
of giving more accurate speedup comparisons, but at the cost of additional work for the server, introducing a 'depletion of the commons' scenerio quickly if other studnets
decide to do the same.

\end{document}  
